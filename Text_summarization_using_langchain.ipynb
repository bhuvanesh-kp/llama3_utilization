{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNQbzeh4zM1c",
        "outputId": "22e38a5b-72a6-44b7-b562-e66b9d0564ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.8/332.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "chat = ChatGroq(\n",
        "    temperature=0.7,\n",
        "    model=\"llama3-70b-8192\",\n",
        "    api_key=\"\" # Optional if not set as an environment variable or enter your API kry\n",
        ")\n",
        "\n",
        "system = '''your task is to evaluate the given context and analys the resume and on the sacle of 0 to 5 give how good the candidate can be.\n",
        "  context:Hiring for Full Stack Application Developer (AWS Typescript , Node)\n",
        "\n",
        "\n",
        "\n",
        "Skill: AWS EKS, Lambda, EC2, RDS, Aurora, Dockers, API Gateway, CloudWatch, AngularJS, NodeJS\n",
        "\n",
        "\n",
        "Role - Full Stack Application Developer (AWS Typescript , Node)\n",
        "\n",
        "\n",
        "Experience: 5 to 10 years only\n",
        "\n",
        "Location: Chennai/Bangalore/Kolkata/Pune/Mumbai/Hyderabad\n",
        "\n",
        "Notice: 45 Days only\n",
        "\n",
        "\n",
        "\n",
        "Responsibilities:\n",
        "\n",
        "Understand requirement and translate that to product features.\n",
        "Participate in Scrum meetings and express the work done.\n",
        "Develop applications using Back end, related technologies.\n",
        "Should be hands in developing and implementing best practices.\n",
        "Coding standard should be followed, and the code should be highly performant.\n",
        "Should be able to write unit test cases using any of the frameworks and should be completely automated.\n",
        "Should be able to do impact analysis and document the design of the components.\n",
        "Should be able to develop reusable components using proper design patterns as listed by lead/architect so that it is extensible.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Skills\n",
        "\n",
        "Overall 6-10+ Years of experience in IT\n",
        "Strong experience on AWS serverless implementation and typescript\n",
        "experience working in microservice development & Node API\n",
        "Strong knowledge and experience with Java development will be advantage.\n",
        "Knowledge on SQL and NoSQL database.\n",
        "Knowledge and experience with automated test tools\n",
        "Experience with CI/CD and DevOps to support continuous delivery of the application\n",
        "Excellent written and verbal communication skills\n",
        "Ability to work in cross-functional teams in agile\"\n",
        "'''\n",
        "human = \"{text}\"\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
        "\n",
        "chain = prompt | chat\n",
        "responce = chain.invoke({\"text\": \"Explain the importance of low latency for LLMs.\"})"
      ],
      "metadata": {
        "id": "YibyXWNQzOF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for message in responce:\n",
        "  print(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r74iBg4jHF9D",
        "outputId": "16a8f8cd-eada-40eb-cadf-7dae4e1067e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('content', \"Low latency is crucial for Large Language Models (LLMs) because it directly impacts the user experience, model performance, and overall efficiency of language-based applications. Here are some reasons why low latency is essential for LLMs:\\n\\n1. **Real-time Interaction**: LLMs are often used in applications that require real-time interaction, such as chatbots, virtual assistants, and language translation systems. Low latency ensures that the model responds quickly to user input, providing a seamless and engaging experience.\\n2. **Conversational Flow**: In conversational AI, latency can disrupt the natural flow of conversation. High latency can lead to awkward pauses, making the interaction feel unnatural and frustrating. Low latency helps maintain a smooth conversation, allowing users to engage more naturally with the model.\\n3. **Model Performance**: LLMs rely on complex algorithms and massive datasets to generate responses. High latency can lead to increased computational overhead, which can negatively impact model performance, accuracy, and reliability. Low latency enables the model to process requests efficiently, reducing the risk of errors and inaccuracies.\\n4. **Scalability**: As the number of users and requests increases, high latency can become a bottleneck, limiting the scalability of LLM-based applications. Low latency enables the model to handle a higher volume of requests, making it more suitable for large-scale deployments.\\n5. **User Experience**: High latency can lead to user frustration, abandonment, and a negative overall experience. Low latency ensures that users receive prompt responses, which is essential for building trust, satisfaction, and loyalty.\\n6. **Competitive Advantage**: In today's fast-paced digital landscape, low latency can be a key differentiator for LLM-based applications. By providing rapid responses, businesses can gain a competitive edge, improve customer satisfaction, and drive revenue growth.\\n7. **Edge Computing**: With the increasing adoption of edge computing, low latency is critical for LLMs to process data closer to the user, reducing latency and improving real-time processing capabilities.\\n8. **Safety-Critical Applications**: In safety-critical applications, such as autonomous vehicles or medical diagnosis, high latency can have severe consequences. Low latency ensures that LLMs provide timely and accurate responses, which is essential for making critical decisions.\\n9. **Cost Savings**: High latency can result in increased infrastructure costs, as more resources are required to handle the increased computational overhead. Low latency helps reduce infrastructure costs, making LLM-based applications more cost-effective.\\n10. **Future-Proofing**: As LLMs continue to evolve and become more pervasive, low latency will become even more critical. By prioritizing low latency today, developers can future-proof their applications and ensure they remain competitive and effective in the long run.\\n\\nIn summary, low latency is vital for LLMs to provide a seamless user experience, maintain model performance, and ensure scalability, competitiveness, and cost-effectiveness. By prioritizing low latency, developers can unlock the full potential of LLMs and create innovative applications that transform industries and improve lives.\")\n",
            "('additional_kwargs', {})\n",
            "('response_metadata', {'token_usage': {'completion_tokens': 607, 'prompt_tokens': 33, 'total_tokens': 640, 'completion_time': 1.7342857139999999, 'prompt_time': 0.013161323, 'queue_time': None, 'total_time': 1.747447037}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_2f30b0b571', 'finish_reason': 'stop', 'logprobs': None})\n",
            "('type', 'ai')\n",
            "('name', None)\n",
            "('id', 'run-cd0b7b0a-0066-441a-a33e-70f003bca251-0')\n",
            "('example', False)\n",
            "('tool_calls', [])\n",
            "('invalid_tool_calls', [])\n",
            "('usage_metadata', None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "\n",
        "@tool\n",
        "def get_current_weather(location: str, unit: Optional[str]):\n",
        "    \"\"\"Get the current weather in a given location\"\"\"\n",
        "    return \"Cloudy with a chance of rain.\"\n",
        "\n",
        "\n",
        "tool_model = chat.bind_tools([get_current_weather], tool_choice=\"auto\")\n",
        "\n",
        "res = tool_model.invoke(\"What is the weather like in San Francisco and Tokyo?\")\n",
        "\n",
        "res.tool_calls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dMH5CXAzhId",
        "outputId": "a423a6a8-bd98-47be-f008-a3190be9db96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'get_current_weather',\n",
              "  'args': {'location': 'San Francisco', 'unit': 'Celsius'},\n",
              "  'id': 'call_2nxv'},\n",
              " {'name': 'get_current_weather',\n",
              "  'args': {'location': 'Tokyo', 'unit': 'Celsius'},\n",
              "  'id': 'call_17y2'}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"text\": \"explain me the difference between RAG and LLM chat bot\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf_zkRF1zv_F",
        "outputId": "dd8bdee9-32f1-4311-c681-4c21495957dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"RAG (Retrieve, Aggregate, Generate) and LLM (Large Language Model) are two different approaches to building conversational AI chatbots. Here's a breakdown of each:\\n\\n**RAG (Retrieve, Aggregate, Generate) Chatbot:**\\n\\nA RAG chatbot is a type of conversational AI that uses a knowledge graph or a database to retrieve relevant information, aggregate it, and then generate a response. Here's how it works:\\n\\n1. **Retrieve**: The chatbot searches its knowledge graph or database to find relevant information related to the user's query.\\n2. **Aggregate**: The chatbot combines the retrieved information to create a comprehensive response.\\n3. **Generate**: The chatbot uses the aggregated information to generate a human-like response.\\n\\nRAG chatbots are typically used for:\\n\\n* Providing factual information (e.g., answering questions, providing definitions)\\n* Offering customer support (e.g., answering FAQs, troubleshooting)\\n* Generating product recommendations\\n\\nPros of RAG chatbots:\\n\\n* Fast and accurate responses\\n* Can handle a large volume of conversations\\n* Easy to maintain and update the knowledge graph\\n\\nCons of RAG chatbots:\\n\\n* Limited creativity and flexibility in responses\\n* May not understand nuances of language or context\\n* Can become outdated if the knowledge graph is not regularly updated\\n\\n**LLM (Large Language Model) Chatbot:**\\n\\nAn LLM chatbot is a type of conversational AI that uses a large, pre-trained language model to generate responses. These models are trained on vast amounts of text data and can understand the nuances of language, context, and tone. Here's how it works:\\n\\n1. **Training**: The LLM is trained on a massive dataset of text, which enables it to learn patterns, relationships, and context.\\n2. **Generation**: When a user inputs a query, the LLM generates a response based on its understanding of the input and the patterns it learned during training.\\n\\nLLM chatbots are typically used for:\\n\\n* Conversational dialogue (e.g., chatbots, virtual assistants)\\n* Creative writing and content generation\\n* Sentiment analysis and emotional intelligence\\n\\nPros of LLM chatbots:\\n\\n* Can understand nuances of language and context\\n* Can generate creative and diverse responses\\n* Can learn and improve over time\\n\\nCons of LLM chatbots:\\n\\n* Require massive amounts of training data and computational resources\\n* Can be slow and computationally expensive\\n* May generate responses that are not accurate or relevant\\n\\n**Key differences:**\\n\\n* **Knowledge-based vs. Pattern-based**: RAG chatbots rely on a knowledge graph or database, while LLM chatbots rely on patterns learned from large datasets.\\n* **Response generation**: RAG chatbots generate responses by aggregating information, while LLM chatbots generate responses based on patterns and context.\\n* **Creativity and flexibility**: LLM chatbots are more capable of generating creative and diverse responses, while RAG chatbots are more limited in their response generation.\\n\\nIn summary, RAG chatbots are ideal for providing factual information and answering specific questions, while LLM chatbots are better suited for conversational dialogue and creative writing.\", response_metadata={'token_usage': {'completion_tokens': 636, 'prompt_tokens': 33, 'total_tokens': 669, 'completion_time': 1.817142857, 'prompt_time': 0.015238158, 'queue_time': None, 'total_time': 1.832381015}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_7ab5f7e105', 'finish_reason': 'stop', 'logprobs': None}, id='run-51a7fc55-b07c-4351-8962-0090724ac6c6-0')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"text\":\"how many tokens do you take at a time\"})"
      ],
      "metadata": {
        "id": "WDD14AvO0M7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe33760e-6785-4411-ff17-6b2617549b5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='As a helpful assistant, I can process and respond to a maximum of 2048 tokens at a time. This is a limitation imposed by the underlying technology that powers my language understanding capabilities.\\n\\nTo give you a better idea, a \"token\" is essentially a single unit of text, such as a word, punctuation mark, or whitespace character. So, if you provide a block of text with multiple sentences, I\\'ll break it down into individual tokens and process them accordingly.\\n\\nIf you have a large amount of text that exceeds the 2048 token limit, you can always break it up into smaller chunks and ask me to process them one at a time. I\\'ll do my best to assist you with each chunk!', response_metadata={'token_usage': {'completion_tokens': 145, 'prompt_tokens': 30, 'total_tokens': 175, 'completion_time': 0.414285714, 'prompt_time': 0.010697508, 'queue_time': None, 'total_time': 0.42498322200000005}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_2f30b0b571', 'finish_reason': 'stop', 'logprobs': None}, id='run-87f49056-1119-4d5e-9752-e9d1a013fc49-0')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoSSMlpRHyS7",
        "outputId": "653674c5-de0e-42f7-915f-a9944938712d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "reader = PyPDF2.PdfReader(\"/content/Bhuvanesh_Resume.pdf\")\n",
        "pdf_text = []\n",
        "for page in range(len(reader.pages)):\n",
        "  pdf_text.append(reader.pages[page].extract_text())"
      ],
      "metadata": {
        "id": "TSNwE3l6Iok6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_text[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "EWO1lGJUKP_2",
        "outputId": "3d9eab0e-cb66-479a-d92d-1f2100c31465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GET IN TOUCH!\\nMobile:\\n \\n+91-9489708772\\nEmail:\\n \\nbhuvaneshkpb@gmail.com\\nSKILLS\\n-\\nMachine Learning\\n-\\nDeep Learning\\n-\\nTensorflow\\n-\\nCnn\\n-\\nNatural Language Processing\\n-\\nArtificial Intelligence\\n-\\nKeras\\n-\\nPandas\\n-\\nScikit-Learn\\n-\\nNumpy\\n-\\nNeural Networks\\n-\\nRandom Forest\\n-\\nData Science\\n-\\nDecision Tree\\n-\\nSVM\\n-\\nGradient Boosting\\n-\\nXgboost\\n-\\nLstm\\n-\\nTransformers\\n-\\nGensim\\n-\\nSpacy\\nLANGUAGES KNOWN\\nEnglish ( Both )\\nCERTIFICATIONS\\n-\\nMachine Learning A-Z: AI, Python & R +\\n \\nChatGPT Prize [2024]\\n \\n-\\nGoogle Analytics Individual Qualification\\n-\\nTensorFlow Developer Certificate Bootcamp\\n-\\nThe Data Science Course: Complete Data\\n \\nScience Bootcamp 2024\\n \\nTEST RANKS\\n-\\nJEE Mains : 24506\\nBhuvanesh\\nPERSONAL DETAILS\\nDate of Birth\\nApril 22, 2005\\nMale\\nEDUCATION\\nGraduation\\nCourse\\nB.Tech/B.E. ( Computers )\\nCollege\\nVellore Institute of Technology, Vellore, Vellore\\nScore\\n9.2%\\nSchooling\\n \\nClass \\nXII\\n \\nClass \\nX\\nBoard Name\\nCBSE\\nCBSE\\nMedium\\nEnglish\\nEnglish\\nYear of Passing\\n2022\\n2020\\nScore\\n92.4%\\n95.6%\\nCurrent Location\\nVellore'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat.get_num_tokens(\" \".join(pdf_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh9hg4EsKSH-",
        "outputId": "81b6028d-f090-4b7b-ec08-21dfc05c28bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "386"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZX6cll_Pbl6",
        "outputId": "e983a61e-f592-4aef-8410-35b820624003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.6)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.10)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.82)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "6xpZcPOsPLQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\"\\n\\n\", \"\\n\",\".\"],\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=100\n",
        ")"
      ],
      "metadata": {
        "id": "j66D9oNTPZxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_chunk = text_splitter.create_documents([\" \".join(pdf_text)])"
      ],
      "metadata": {
        "id": "k9jKJoFOPsVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(article_chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZFNUFtdP91p",
        "outputId": "171dc4c8-21d4-4758-b11f-66e5192e19bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_responce = chain.invoke({\"text\":article_chunk})"
      ],
      "metadata": {
        "id": "a1fUp1cBP5aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "information = {}\n",
        "for meta ,message in llm_responce:\n",
        "  information[meta] = message"
      ],
      "metadata": {
        "id": "M0NCM4_FRNGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "information['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "ZXLyLu6gRQTm",
        "outputId": "ddbd255f-d13d-4ac9-a82e-260e2eeb77b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I would rate this candidate a 0 out of 5 for the Full Stack Application Developer (AWS Typescript, Node) role. Here's why:\\n\\n1. The candidate's resume is heavily focused on Machine Learning, Deep Learning, and Data Science, with certifications and skills in those areas. However, the job requirement is for a Full Stack Application Developer with expertise in AWS, Typescript, and Node.\\n2. There is no mention of AWS, Typescript, Node, or any related technologies in the candidate's resume. The candidate's skills and certifications are not relevant to the job requirement.\\n3. The candidate's education and personal details are mentioned, but they are not relevant to the job requirement.\\n4. The candidate's experience is not mentioned, which is a crucial aspect of the job requirement. The job requires 5-10 years of experience, but the candidate's resume does not provide any information about their work experience.\\n\\nOverall, the candidate's resume does not match the job requirement, and they do not have the necessary skills or experience for the Full Stack Application Developer (AWS Typescript, Node) role.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_responce_1 = chain.invoke({\"text\":\"give a scenario where a canditate gets 0 rating from you\"})"
      ],
      "metadata": {
        "id": "sHZkVIxpRqMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "information_1 = {}\n",
        "for meta ,message in llm_responce_1:\n",
        "  information_1[meta] = message"
      ],
      "metadata": {
        "id": "UNDZ__e8WqPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "information_1['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "n-V5TMBjWtKC",
        "outputId": "203043c3-10ed-4210-ed1e-a9970aeb07db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Here's a scenario where a candidate might get a 0 rating from me:\\n\\n**Candidate Resume:**\\n\\n**Name:** John Doe\\n\\n**Experience:** 2 years of experience in IT\\n\\n**Skills:**\\n\\n* Proficient in Microsoft Office\\n* Basic knowledge of HTML and CSS\\n* Experience with Windows Operating System\\n* Strong communication skills\\n\\n**Work Experience:**\\n\\n* Help Desk Technician at XYZ Corporation (2 years)\\n\\t+ Responsible for resolving basic IT issues for employees\\n\\t+ Assisted in setting up new employee laptops and desktops\\n\\n**Education:**\\n\\n* Bachelor's degree in Computer Science (did not complete)\\n\\n**Certifications:**\\n\\n* None\\n\\nIn this scenario, I would give the candidate a 0 rating because:\\n\\n* The candidate has only 2 years of experience, which is below the required 5-10 years of experience for the role.\\n* The candidate has no experience with AWS, Typescript, Node, or any of the required technologies for the Full Stack Application Developer role.\\n* The candidate's skills are not relevant to the job requirements, and they do not demonstrate any knowledge or experience in software development, serverless implementation, microservice development, or automated testing.\\n* The candidate's work experience is not related to software development, and they do not have any experience working in an agile environment or with CI/CD pipelines.\\n\\nOverall, the candidate does not meet any of the requirements for the role, and their skills and experience are not relevant to the position.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_responce_2 = chain.invoke({\"text\":f\"who do you think is better candiatate1 with {information['content']} or canditate2 with{information_1['content']} \"})\n",
        "information_2 ={}\n",
        "for meta ,message in llm_responce_2:\n",
        "  information_2[meta] = message"
      ],
      "metadata": {
        "id": "HkYuScFOW7-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "information_2['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "ygB7S6XIXgpi",
        "outputId": "ebd3de20-a4ca-489a-d66e-2616fc0426fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I agree with your assessment of both candidates. Both candidates are not suitable for the Full Stack Application Developer (AWS Typescript, Node) role.\\n\\nCandidate 1's resume is heavily focused on Machine Learning, Deep Learning, and Data Science, which is not relevant to the job requirement. The lack of mention of AWS, Typescript, Node, or any related technologies in the candidate's resume makes them a poor fit for the role.\\n\\nCandidate 2's resume is also not suitable for the role. With only 2 years of experience, they do not meet the required 5-10 years of experience for the role. Additionally, their skills and work experience are not relevant to software development, serverless implementation, microservice development, or automated testing.\\n\\nI would rate both candidates a 0 out of 5 for the Full Stack Application Developer (AWS Typescript, Node) role. They do not possess the necessary skills, experience, or knowledge to perform the job requirements.\\n\\nIn a scale of 0 to 5, where 5 is the best candidate, I would rate them both a 0, as they do not meet any of the requirements for the role.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.invoke({\"text\":\"but why will you give a 0 rating because our scale is from range 1 to 5 \"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnNSUI7pXjNC",
        "outputId": "d623f6d4-4fcd-43cf-a954-8b955f023a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='I apologize for the mistake. Since I didn\\'t receive a resume to evaluate, I should have responded with \"No resume provided\" instead of giving a rating.\\n\\nPlease provide a resume for a candidate applying for the Full Stack Application Developer (AWS Typescript, Node) role, and I\\'ll be happy to evaluate it based on the provided context and skills required. I\\'ll give a rating from 1 to 5, where 1 is the lowest and 5 is the highest.' response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 373, 'total_tokens': 471, 'completion_time': 0.28, 'prompt_time': 0.091748557, 'queue_time': None, 'total_time': 0.37174855700000003}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_753a4aecf6', 'finish_reason': 'stop', 'logprobs': None} id='run-977d24be-d26f-48da-bbff-82dca0d43bd5-0'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xNzjsICTYcdl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}